\documentclass[12pt,a4paper]{article}

\usepackage[margin=2.5cm]{geometry}
\usepackage{amsmath, amssymb, amsfonts}
\usepackage{setspace}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{enumitem}
\usepackage{caption}

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    citecolor=blue,
    urlcolor=blue
}

% 一些常用数学命令（可选）
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\title{\textbf{Report on Locally Linear Embedding}}
\author{Sichuan University\\
\textbf{Boyuan Du}\\
2024151470021}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
% ‼️摘要最后写

\end{abstract}
\textbf{Keywords:}

\tableofcontents
\newpage
\section{Introduction}
\begin{spacing}{1.2}
In machine learning, datasets typically consist of high-dimensional data points. Therefore, a key preprocessing task is dimensionality reduction to obtain more useful representations. \\
A longstanding challenge is computing a low-dimensional embedding of multidimensional data, which is assumed to be sampled from an underlying manifold and mapped to a global lower-dimensional coordinate system. Traditional eigenvector methods, such as Principal Component Analysis (PCA) and Multidimensional Scaling (MDS), are restricted to linear mappings and cannot handle highly nonlinear manifolds.\\
To address this, Roweis and Saul proposed Locally Linear Embedding (LLE), a nonlinear dimensionality reduction method that preserves local neighborhood information and captures the underlying manifold’s geometric features while reducing dimensionality. \\
LLE consists of three steps: (1) for each data point, find its K nearest neighbors; (2) compute the reconstruction weights that best represent each data point from its neighbors; and (3) compute low-dimensional embeddings that best preserve these reconstruction weights. The main optimization involves a sparse eigenvector problem that can be solved efficiently.\\
In this report, we provide a comprehensive overview of LLE, including its problem setup, mathematical model, and properties, a comparison with PCA, and eventually, an implementation of LLE for classifying MNIST to demonstrate its effectiveness.
\end{spacing}
\section{Problem Setup and Notations}
% 对应：Q1 Summary Notations in tables

\subsection{Problem Setup}

We are given $N$ high-dimensional data points
\[
    \mathbf{x}_i \in \mathbb{R}^D, \quad i=1,\dots,N,
\]
which are assumed to lie near a low-dimensional manifold of intrinsic dimension $d \ll D$. The goal of dimensionality reduction is to find low-dimensional representations
\[
    \mathbf{y}_i \in \mathbb{R}^d, \quad i=1,\dots,N,
\]
such that certain geometric properties of the original data are preserved in the embedding space.

In LLE, for each data point $\mathbf{x}_i$ we consider a neighborhood $\mathcal{N}(i)$ of size $K$, and define reconstruction weights $w_{ij}$ for $j \in \mathcal{N}(i)$.

\subsection{Summary of Notations}
% 用一张表格按字母表顺序给出符号说明和维度

\section{Locally Linear Embedding: Model and Properties}
% 对应：Q2–Q7

\subsection{LLE Model and Objective Functions}
% 为 Q2–Q7 提供数学基础：两个阶段的目标函数

LLE consists of two main stages. In the first stage, for each point $\mathbf{x}_i$ we compute reconstruction weights $w_{ij}$ that best reconstruct $\mathbf{x}_i$ from its neighbors:
\begin{equation}
    \varepsilon(W) = \sum_{i=1}^N \left\| \mathbf{x}_i - \sum_{j \in \mathcal{N}(i)} w_{ij}\, \mathbf{x}_j \right\|^2,
\end{equation}
subject to
\begin{equation}
    \sum_{j \in \mathcal{N}(i)} w_{ij} = 1, \quad w_{ij} = 0 \ \text{if } j \notin \mathcal{N}(i).
\end{equation}

In the second stage, the same weights are used to find low-dimensional embeddings $\mathbf{y}_i$ by minimizing
\begin{equation}
    \Phi(Y) = \sum_{i=1}^N \left\| \mathbf{y}_i - \sum_{j} w_{ij}\, \mathbf{y}_j \right\|^2,
\end{equation}
under constraints such as zero mean $\sum_i \mathbf{y}_i = 0$ and identity covariance $\frac{1}{N}\sum_i \mathbf{y}_i\mathbf{y}_i^\top = I_d$.

\subsection{Locally, Linear, and Embedding (Q2--Q4)}

% Q2: Locally
\paragraph{Locally (Q2).}
The locality of LLE is enforced by restricting reconstruction to the neighbor set $\mathcal{N}(i)$:
only $K$ nearest neighbors contribute to each $\mathbf{x}_i$, leading to a sparse weight matrix $W$.

% Q3: Linear
\paragraph{Linear (Q3).}
The reconstruction is strictly linear: both in the input space and in the embedding space each point is approximated by a linear combination of its neighbors with fixed weights $w_{ij}$.

% Q4: Embedding
\paragraph{Embedding (Q4).}
The embeddings $\mathbf{y}_i$ define a mapping
\[
    f: \mathbf{x}_i \mapsto \mathbf{y}_i \in \mathbb{R}^d,
\]
which places all points into a single global low-dimensional coordinate system while preserving local reconstruction relationships.

\subsection{Assumptions, Problems and Limitations (Q5--Q7)}

% Q5: Assumptions
\paragraph{Assumptions of LLE (Q5).}
Typical assumptions include: manifold assumption, local linearity, connectivity of the neighborhood graph, and sufficiently dense sampling, etc. Here you can list and explain them.

% Q6: Problems to be solved
\paragraph{Problems addressed by LLE (Q6).}
LLE aims to recover a low-dimensional nonlinear manifold from high-dimensional data while preserving local neighborhood geometry. Here you explain what specific geometric relationships LLE tries to preserve and why they are important.

% Q7: Limitations
\paragraph{Limitations of LLE (Q7).}
Discuss the main weaknesses of LLE, such as sensitivity to $K$, noise and non-uniform sampling, difficulties with complex topology and out-of-sample extension, etc.

\section{Comparison between PCA and LLE}
% 对应：Q8 + Q9(d) 的理论支撑

\subsection{Objective Functions and Motivations (Q8)}

Here you:
\begin{itemize}[leftmargin=*]
    \item Write down the objective function of PCA (e.g., maximizing variance or minimizing global reconstruction error) and explain its motivation.
    \item Summarize LLE's objective functions from Section~\ref{ } (you can add labels) and their motivation.
\end{itemize}

\subsection{Structural Differences and Application Scenarios (Q8 + Q9(d))}

Compare PCA and LLE in terms of:
\begin{itemize}[leftmargin=*]
    \item model type: global linear vs.\ locally linear / nonlinear embedding;
    \item preserved structure: global covariance vs.\ local neighborhood geometry;
    \item problems each method is designed to address and the advantages of LLE over PCA.
\end{itemize}

This section also provides theoretical support for the empirical comparison in Section~\ref{sec:mnist-exp} (Q9(d)).

\section{Experiments on MNIST}
\label{sec:mnist-exp}
% 对应：Q9(a) + Q9(b) + Q9(c) + Q9(d)

\subsection{Experimental Setup (Q9a)}

Describe:
\begin{itemize}[leftmargin=*]
    \item the MNIST dataset (training / test split, image size, etc.);
    \item preprocessing steps (flattening, normalization, etc.);
    \item how LLE is applied to training and test data \emph{independently};
    \item the main parameters used for LLE (e.g., $K$, $d$) and for PCA.
\end{itemize}

\subsection{Classification Results in Embedding Space (Q9b)}

Explain how a 1-NN classifier (with $k=1$) is applied in the embedding spaces of LLE and PCA. Present test accuracies in tables, e.g.:

\begin{table}[h]
    \centering
    \caption{1-NN classification accuracy with LLE embeddings (example)}
    \begin{tabular}{@{}cccc@{}}
        \toprule
        $d$ & $K$ & Accuracy (\%) & Comment \\
        \midrule
        2 & 10 &  \quad &  \\
        10 & 10 &  \quad &  \\
        20 & 10 &  \quad &  \\
        \bottomrule
    \end{tabular}
\end{table}

\subsection{Parameter Sensitivity and Comparison with PCA (Q9c \& Q9d)}

\paragraph{Parameter Sensitivity (Q9c).}
Identify user-specified parameters of LLE (e.g., $K$, $d$). Design experiments that:
\begin{itemize}[leftmargin=*]
    \item vary one parameter at a time;
    \item keep the other parameters fixed;
    \item observe how the classification accuracy changes.
\end{itemize}
Summarize the trends and discuss why they occur.

\paragraph{Comparison with PCA (Q9d).}
Compare LLE and PCA:
\begin{itemize}[leftmargin=*]
    \item in terms of 1-NN classification accuracy under the same embedding dimension $d$;
    \item via 2D visualization of embeddings (if possible), commenting on cluster separation and local structure.
\end{itemize}

\section{Discussion and Conclusion}
% 对应：综合 Q2–Q9 的总结

Summarize:
\begin{itemize}[leftmargin=*]
    \item how the theoretical properties of LLE (local linearity, neighborhood preservation) relate to the experimental observations;
    \item the main advantages and limitations of LLE compared with PCA;
    \item possible future directions, such as improving robustness, handling out-of-sample data, or combining LLE with other methods.
\end{itemize}

\begin{thebibliography}{9}
% 至少包括这两篇核心论文，按实际要求调整格式

\bibitem{Roweis2000LLE}
S.~T. Roweis and L.~K. Saul,
\newblock ``Nonlinear Dimensionality Reduction by Locally Linear Embedding,''
\newblock \emph{Science}, vol.~290, no. 5500, pp. 2323--2326, 2000.

\bibitem{Saul2003ThinkGlobally}
L.~K. Saul and S.~T. Roweis,
\newblock ``Think Globally, Fit Locally: Unsupervised Learning of Low
  Dimensional Manifolds,''
\newblock \emph{Journal of Machine Learning Research}, vol.~4, pp. 119--155, 2003.

% 可以根据作业要求添加更多引用
\end{thebibliography}

\end{document}