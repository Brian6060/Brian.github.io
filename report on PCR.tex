\documentclass[11pt,a4paper]{article}

% ---------- Packages ----------
\usepackage[margin=1in]{geometry}
\usepackage{subcaption}
\usepackage{needspace}
\usepackage{amsmath,amssymb,mathtools,bm}
\usepackage{microtype}
\usepackage{setspace}
\usepackage{graphicx}
\usepackage{booktabs,multirow}
\usepackage{enumitem}
\usepackage{hyperref}
\usepackage[capitalise,noabbrev]{cleveref}
\usepackage{listings}
\usepackage{titling}
\usepackage[T1]{fontenc}
\usepackage{tabularx}
\usepackage{array} 
\usepackage{placeins}

% --- common math operators/macros ---
\newcommand{\R}{\mathbb{R}}
\DeclareMathOperator{\rank}{rank}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\operatorname{Var}}
\newcommand{\Cov}{\operatorname{Cov}}
\newcommand{\trace}{\operatorname{tr}}
\newcommand{\diag}{\operatorname{diag}}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

% --- vectors/matrices (bold) ---
\newcommand{\vx}{\mathbf{x}}
\newcommand{\vX}{\mathbf{X}}
\newcommand{\vy}{\mathbf{y}}
\newcommand{\vY}{\mathbf{Y}}
\newcommand{\vC}{\mathbf{C}}
\newcommand{\vP}{\mathbf{P}}
\newcommand{\vU}{\mathbf{U}}
\newcommand{\vV}{\mathbf{V}}
\newcommand{\vE}{\mathbf{E}}
\newcommand{\vSigma}{\bm{\Sigma}} 
\newcommand{\vp}{\mathbf{p}}

% --- paired delimiters ---
\newcommand{\norm}[1]{\left\lVert #1 \right\rVert}
\newcommand{\inner}[2]{\left\langle #1,\, #2 \right\rangle}

\newcolumntype{Y}{>{\raggedright\arraybackslash}X} 

% ---------- Safe helpers ----------
\makeatletter
\newcommand{\maybeincludegraphics}[2][]{%
  \IfFileExists{#2}{\includegraphics[#1]{#2}}{%
    \fbox{\parbox[c][4cm][c]{0.9\linewidth}{\centering
      Placeholder: \texttt{#2} not found}}}}
\newcommand{\maybelstinput}[2][]{%
  \IfFileExists{#2}{\lstinputlisting[#1]{#2}}{%
    \noindent\emph{(File \texttt{#2} not found; listing omitted.)}}}
\makeatother

% ---------- Lists ----------
\setlist[itemize]{noitemsep,topsep=3pt}
\setlist[enumerate]{noitemsep,topsep=3pt}

% ---------- Title ----------
\pretitle{\vspace*{-2em}\begin{center}\LARGE\bfseries}
\posttitle{\par\end{center}\vspace{0.5em}}
\preauthor{\begin{center}\large}
\postauthor{\par\end{center}\vspace{0.5em}}
\predate{\begin{center}\small}
\postdate{\par\end{center}}

\title{Reading Report:\\ \emph{A Tutorial on Principal Component Analysis}}
\author{Boyuan Du (2024151470021)\\
School of Software,\textbf{ Sichuan University}\\
{Email:2024151470021@stu.scu.edu.cn}}
\date{\today}

% ---------- Document Formatting ----------
\setlength{\parindent}{0pt}    % 取消首行缩进
\setlength{\parskip}{0.8em}    % 段落间距

% 避免标题与正文分隔
\usepackage[compact]{titlesec}
\titlespacing*{\section}{0pt}{*2}{0.5em}
\titlespacing*{\subsection}{0pt}{*1.5}{0.3em}
\titlespacing*{\paragraph}{0pt}{*1}{0.2em}

% 避免孤行寡行
\clubpenalty=10000
\widowpenalty=10000
\displaywidowpenalty=10000

\begin{document}
\maketitle

\noindent\textbf{Course / Assignment:} Reading report on PCA .\\
\textbf{Primary Source:} J.~Shlens, ``A Tutorial on Principal Component Analysis,'' 2014.

\begin{abstract}
PCA is a powerful and fundamental technique for extracting useful information from complex(high-dimensional) data by lowering dimension onto basic features of the data. The essence of PCA lies in its ability to identify the directions of maximum variance in the data, which are represented by the principal components. By projecting the data onto these components, PCA effectively reduces redundancy and highlights the most informative aspects of the dataset. The tutorial emphasizes two main approaches to PCA: eigen-decomposition (ED) of the covariance matrix and singular value decomposition (SVD) of the data matrix. Both methods yield same results with the help of eigenvector. However they has a common weakness that they rely on the assumption that principal components are orthogonal which limits the two techniques to extract non-vertical principal component, but Independent Component Analysis can handle this problem efficiently. "about the experiment of MNIST..."
\\
\\
\\
\\\textbf{Keywords:} PCA, covariance, eigen-decomposition, SVD, variance.
\end{abstract}
\clearpage

\tableofcontents
\vspace{0.5em}
\clearpage


% =========================
% ========== A. Problem & Setting (4.(0/1/3)) ==========
\section{Foundation \& Settings}
\label{sec:problem}
\paragraph{}
Principal Component Analysis(PCA) is a technique to extract useful and hiden information and knowlegde form confuse and big datasets. It completely bases on linear algebra to achieve the goal of dimension reduction of data, in which, the vector and matrix is crucial and foundamental elements. Like the example of a flunctuate spring, PCA can extract a main movement in a single axis $x$ from three different path record in different angle which is both noisy and redundant. A matrix in PCA can serve multiple roles such as a 
\begin{equation}
X=[x^{(1)}\ \cdots\ x^{(n)}]\in\mathbb{R}^{m\times n}
\end{equation}
In which each column is a sample and each row reflect a dimension of datasets,

and \textbf{linear transformation} as stretch or rotation and projection:
\begin{equation}
C_Y = P C_X P^\top
\end{equation}
$P$ projects $C_X$ onto a new basis where the off-diagonal elements equal to zero.
\\In the process of PCA, we measure the dataset from different directions orthonormally and find the direction with largest variance which is called \textbf{principal component(PC)}. So in linear algebra, we use a set of orthonormal vertors as basis, to reveal the the way we measure the data.
The essence of PCA is to find a linear transformation $P$ to project the data $X$ onto a new basis $Y=PX$ where the covariance $C_Y$ becomes a diagnoal matrix in order to remove the noise and redundancy in raw data material.

% ========== B. Covariance, Redundancy & SNR ==========

\section{Covariance, Redundancy, and SNR}

\subsection{Covariance}
\label{sec:Covariance}
\paragraph*{Intuitive Definition of Covariance}\mbox{}\\
Covariance represents the relationship between two variables. In PCA, it reflects the the degree of different dimensions of datasets overlapped which we called \emph{Redundancy}. The larger the covariance is , the more two dimensions are correlated and overlapped which means bigger redundancy.
\begin{equation}
C_X=\tfrac{1}{n}\,X X^T,\qquad (C_X)_{ij}=\tfrac{1}{n}\sum_{\ell=1}^n X_{i\ell}\,X_{j\ell}.
\end{equation}

\subsection{Covariance, Redundancy, and Noise}
\paragraph*{}\mbox{}\\In contrast of covariance, the diagonal elements of $C_X$ represent the variance of each dimension which reflects the useful information of the data we call this direction\emph{Signal}. And the emph{noise} is the opposite infomation in the direction vertical to the signal.
So we first divide the covariance matrix into two parts: diagonal elements and off-diagonal elements. The former consists of the variance of signal direction and the noise direction. The latter consists of the covariance between different dimensions.
Now it is clear that PCA aims to reduce both the irrelavant information(noise) and the repetitive information(redundancy) by finding a new basis to project the data onto where the covariance matrix becomes diagonal and the variance along each dimension is ordered from large to small.


\subsection{SNR \& Variance}
\paragraph*{}\mbox{}\\
Before we take steps to decrease the noise, we measure the quality of data by compare the variance between signal and noise. So logically, we define the \emph{Signal-to-Noise Ratio(SNR)} as:
\begin{equation}
\mathrm{SNR} = \frac{\sigma^2_{\text{signal}}}{\sigma^2_{\text{noise}}}
\end{equation}
where $\sigma^2_{\text{signal}}$ is the variance along the signal direction and $\sigma^2_{\text{noise}}$ is the variance along the noise direction. A higher SNR indicates a cleaner and more reliable measurement, while a lower SNR suggests that the data is more contaminated by noise.
In PCA, we make effort to increase the SNR by seeking directions where $\sigma^2_{\text{noise}}$ closes in on zero.

% ========== C. Assumptions & Limits (4.(2)) ==========
\section{Assumptions \& Limits of PCA}
\label{sec:Ass-Lim}
\paragraph*{I. Linearity}\mbox{}\\
PCA assumes a \textbf{linear change of basis}. With dataset $X\in\mathbb{R}^{m\times n}$ and covariance $C_X=\tfrac{1}{n}XX^\top$, it seeks a linear transformation $P$ to project $X$ onto a new basis $Y=PX$ where the covariance $C_Y=\tfrac{1}{n}YY^\top$ becomes diagonal. 
\begin{equation}
Y = PX, \quad
P \in O(m),
\quad P^\top P = I_m.
\end{equation}
However, this assumption restricts PCA to only extract linear features from data, but Independent Component Analysis(ICA) can further tackle this problem efficiently.
\paragraph*{II. Large Variance = Important Structure}\mbox{}\\
PCA assumes that directions with largest variance correspond to the most important underlying structure which contains the most valuable information.
We assume that the useful information is embedded in direction $G$, so this assumption is:
\begin{equation}
\forall g_{ij}\in G,\ \sigma^2 = \mathbf{g}_{ij} C_X \mathbf{g}_{ij}^\top \to \infty
\end{equation}
However, this assumption may not always be correct, and in real cases, this often leads to a wrong direction.

\paragraph*{III. The principal components are orthogonal}\mbox{}\\
PCA assumes that the principal components are orthogonal to each other. Now we demonstrate this assumption in the process of PCA:
PCA seeks a linear transformation $P$ to project $X$ onto a new basis $Y=PX$ where

\begin{equation}
C_Y = P C_X P^\top
\end{equation}

becomes diagonal. And we assume that:
\begin{equation}
P\in O(m),(p_i^\top p_j=0, \forall i\neq j)\ \\\ P^\top P=I_m(\forall i=j, p_i^\top p_j=1)
\end{equation}
This assumption is crucial because it ensures that the principal components are uncorrelated, which simplifies the analysis and interpretation of the data. However, it also limits the ability of PCA to capture non-orthogonal features in the data.

% ========== D. PCA as Change of Basis (4.(6/7/8/9)) ==========
\section{PCA as a Change of Basis}
\label{sec:pca-cob}
\paragraph{Change-of-Basis Formulation.}
\begin{equation}
  \vy = \vP \vx, \qquad \vC_Y = \vP \vC_X \vP^\top
\end{equation}
Principal components is the process where we change the basis of the data matrix; it is the very way we change our directions of measurement and evaluation on the data, reducing the noise and redundancy to approach the best condition. So the principal component is embedded in the matrix $\vP$ that makes $\vC_Y$ diagonal.
Every row of $\vP$ is a principal component, and the variance along that component is given by the corresponding diagonal element of $\vC_Y$.

\paragraph{Projection and Re-expression}\mbox{}\\
The way we re-express the data in the new basis is by projecting the data onto each principal component. The projection of a data point $x$ onto a principal component $p_i$ is produced by the dot product $\inner{\vp_i}{\vx}$. This projection gives us the coordinate of the data point in the new basis along that principal component.
And we visualize it as the rotation and stretch of the basis vectors.

\begin{figure}[htbp]
  \centering
  % 第一张图
  \begin{subfigure}{0.45\linewidth}
    \centering
    \includegraphics[width=\linewidth]{data_raw.png}
    \caption{Raw data with a random basis}
    \label{fig:raw}
  \end{subfigure}
  \hfill
  % 第二张图
  \begin{subfigure}{0.45\linewidth}
    \centering
    \includegraphics[width=\linewidth]{data_pca.png}
    \caption{After PCA}
    \label{fig:pca}
  \end{subfigure}
  \caption{Comparison of data before and after PCA.}
  \caption{figure(a) shows the raw data in a random basis,where the axes are not aligned with the data's main variance direction, leading to a noisy and redundant representation. Figure (b) illustrate the same data after PCA, where the axes are aligned with the principal components, resulting in a clearer and more structured representation.}
  \label{fig:data-compare}
\end{figure}


\paragraph{Relation to Covariance}\mbox{}\\
PCA is a linear transformation that re-expresses the dataset in a new orthonormal basis so that the covariance matrix becomes diagonal. In this process, the off-diagonal which represent redundancy between variables are eliminated, and the diagonal elements is the variances along the principal components. 
In conclusion, PCA is a tool to change the basis of data, and the covariance matrix is a measure of how well the data is represented in that basis. 
\label{sec:edsvd}


% ========== E. ED \& SVD Routes (4.(10/11)) ==========
\section{Two powerful route to PCA: ED \& SVD}
\label{sec:edsvd}
\paragraph{ED Route \emph{Eigen decomposition}}\mbox{}\\
As we mentioned before, PCA seeks a linear transformation $\vP$ to project $\vX$ onto a new basis $\vY=\vP\vX$ where the covariance $\vC_Y=\tfrac{1}{n}\vY\vY^\top$ becomes diagonal. 
In linear algebra, we know that a symmetric matrix can be diagonalized by its eigenvectors. So we can find the eigenvectors of the covariance matrix $\vC_X=\tfrac{1}{n}\vX\vX^\top$ to form the projection matrix $\vP$. The eigenvectors of $\vC_X$ are orthogonal and can be used as the new basis for the data. 
Intuitively, the eigenvectors represent the directions of maximum variance in the data, and the corresponding eigenvalues represent the amount of variance along those directions. By projecting the data onto the eigenvectors, we can effectively reduce redundancy and highlight the most informative aspects of the dataset.

\begin{equation}
  \vC_X = \vE \mathbf{D} \vE^\top, \quad \text{PCs} \equiv \text{columns of } \vE,\; \text{variance} \equiv \text{diag}(\mathbf{D})
\end{equation}

\paragraph{SVD Route \emph{Singular Value Decomposition}}\mbox{}\\
Another viable way to PCA is to use the Singular Value Decomposition(SVD) of the data matrix $\vX$. 
The left singular vectors $u_i$ and right singular vectors $v_i$ of $\vX$ are the eigenvectors of $\vX\vX^\top$ and $\vX^\top\vX$ respectively. The singular values $\sigma_i$ are the square roots of the eigenvalues of $\vX^\top\vX$.
SVD decomposes $\vX$ into three matrix: $\vU$, $\vSigma$, and $\vV^\top$. The columns of $\vV$ are the right singular vectors of $\vX$, which correspond to the principal components. The diagonal elements of $\vSigma$ are the singular values, which are related to the variance along each principal component.


\begin{equation}
  \vX = \vU \vSigma \vV^\top, \quad \text{PCs} \equiv \text{columns of } \vV
\end{equation}
Till now, it's clear and easy to see the pros and cons of the two methods:
\textbf{ED} is more intuitive and directly related to the covariance matrix, but the realization of \textbf{ED} requries a couple of extra conditions of data matrix that it must be square and symmetric.
\textbf{SVD} is more general and can deal with any data matrix, but it is less intuitive and the features of decomposing data into three matirx requires a surge of computation.

% ========== F. Objective & Why DR Works (4.(12/13)) ==========
\section{The Viability of PCA in Dimensional Reduction}
PCA as it's name suggests, is a method to find the principal components of the data. But the way we define "\textit{principal}" is not so clear. In the context of dimensional reduction, the degree to which a reduced representation can predict the original data is a how we define success and that reveals how principal the components we use to simplify the data are.
Quantitatively, we use the \textbf{Mean Squared Error(MSE)} to measure the difference between the original data and the predict the data from the simplified representation.
\begin{equation}
  \text{MSE} = \frac{1}{n} \sum_{i=1}^n \| x^{(i)} - \hat{x}^{(i)} \|^2
\end{equation}
To minimize the MSN, we need to maximize the variance along the principal components we choose to keep, and the mathmatical result is that we need to choose the top $k$ eigenvectors of the covariance matrix $\vC_X$ or the top $k$ right singular vectors of the data matrix $\vX$, just as we do in PCA.
\begin{equation}
  \text{MSE} = \sum_{i=k+1}^m \lambda_i
\end{equation}
So PCA is a viable method for dimensional reduction because it effectively captures the most important features of the data while minimizing the loss of information.
% =========================
% ========== G. Conclusion (4.(4/5)) ==========
\section{Conclusion}
PCA is a powerful and fundamental technique for extracting useful information from complex and high dimensional data by transform the data into a new basis where the covariance matrix becomes diagonal. The essence of PCA lies in its ability to identify the directions of maximum variance in the data, which are represented by the principal components. By projecting the data onto these components, PCA effectively reduces redundancy and highlights the most informative aspects of the dataset. And the way ED and SVD proved to be a viable and powerful method to achieve PCA as well as dimensional reduction. However, PCA has some limitations, such as its reliance on the assumption that principal components are orthogonal. In cases where these assumptions do not hold, substitute techniques such as Independent Component Analysis (ICA) may be more appropriate.
\clearpage

\appendix
\begin{table}[h]
\section{Mathmatical Notations}\label{sec:mathmatical notations}
\caption{Alphabetical summary of mathematical notations used in the PCA tutorial}
\begin{tabular}{@{}l p{0.42\textwidth} p{0.36\textwidth}@{}}
\toprule
\textbf{Notation} & \textbf{Definition} & \textbf{Corresponds to} \\
\midrule
$A,\,B$ & Two general matrix used to define or explain other definitions below & tool matrix \\
$a_i,\,b_i$ & $i$-th samples of $A$ and $B$ & scalar observations in demonstrating example \\
$C_X=\tfrac{1}{n}XX^{\top}$ & Covariance matrix of $X$  & reveals the redundancy of dataset X  \\
$C_Y=\tfrac{1}{n}YY^{\top}$ & Covariance of $Y=PX$ under a new basis & covariance matrix after change of basis\\
$D$ & Diagonal matrix of eigenvalues in eigendecomposition & variances along principal components \\
$E$ & Matrix whose columns are eigenvectors of $C_X$ & principal directions \\
$I$ & Identity matrix & orthonormal basis in $\mathbb{R}^m$ \\
$k$ & Target dimension for reduction($Xa=kb$in the explaination of SVD) &  \\
$m$ & Number of features (measurement types) & dimensions of dataset \\
$n$ & Number of samples (trials) & scale of training set \\
$P=[p_1^{\top}\!\cdots p_m^{\top}]$ & rotation and stretch to transforms$X$into$Y$ & projection matrix\\
$p_i$ & $i$-th principal component (row of $P$) & principal axis \\
$r$ & Rank of $X$ (or $X^{\top}X$) & intrinsic dimensionality \\
$\mathrm{SNR}=\sigma^2_{\text{signal}}/\sigma^2_{\text{noise}}$ & Signal-to-noise ratio & measurement quality \\
$\Sigma=\diag(\sigma_1,\ldots,\sigma_r)$ & Diagonal matrix of singular values & covarience in new basis \\
$\sigma_i$ & $i$-th singular value of $X$; $\lambda_i=\sigma_i^2/n$ (if $C_X=\tfrac1n XX^{\top}$) & scale of mode $i$ \\
$\sigma^2$ & Variance of a scalar variable/sequence & spread/energy \\
$U$ & Left singular vectors of $X$ & orthonormal basis of column space \\
$V$ & Right singular vectors of $X$ & orthonormal basis of row space \\
$\hat u_i$ & $i$-th left singular vector; $\hat u_i=\tfrac{1}{\sigma_i}X\hat v_i$ & output direction of mode $i$ \\
$\hat v_i$ & $i$-th eigenvector of $X^{\top}X$ & input direction of mode $i$ \\
$X\in\mathbb{R}^{m\times n}$ & Data matrix  & stacked measurements dataset \\
$x^{(j)}$ & $j$-th sample vector (a column of $X$) & per-sample measurement \\
$Y=PX$ & Data expressed in PCA coordinates & projections onto PCs \\
$Z=U^{\top}X$ & Coordinates in the left-singular basis & transformed data \\
$\lambda_i$ & $i$-th eigenvalue of $C_X$ & variance along the $i$-th PC \\
$\delta_{ij}$ & element $U$($=1$ if $i=j$, else $0$) & orthogonality indicator \\
$\|\cdot\|$ & Euclidean norm & vector length \\
$(\cdot)^{\top}$ & Transpose & matrix transpose \\
$\cdot$ & Dot product & inner product \\
\bottomrule
\end{tabular}
\end{table}
\FloatBarrier


% =========================

\begin{thebibliography}{9}
\bibitem{Shlens2014}
J.~Shlens, \emph{A Tutorial on Principal Component Analysis}, 2014.
\end{thebibliography}


\end{document}